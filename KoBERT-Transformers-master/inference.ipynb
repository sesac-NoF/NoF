{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\nof\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\anaconda3\\envs\\nof\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--monologg--kobert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\user\\anaconda3\\envs\\nof\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--monologg--distilkobert. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, DistilBertModel\n",
    "bert_model = BertModel.from_pretrained('monologg/kobert')\n",
    "distilbert_model = DistilBertModel.from_pretrained('monologg/distilkobert')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda 가상환경 터미널에서 새로운 가상환경을 만들고 활성화 하신뒤 pip install -r requirements.txt를 먼저해주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   2, 3302, 4642, 6043, 6903, 2569, 5682, 3916, 6855, 6116, 1467, 7074,\n",
      "         6586,   54,    3],\n",
      "        [   2, 1567, 3916, 6855, 2120, 7187, 4542, 7814,   54,    3,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [   2, 3647, 2120, 7186, 4736, 7003, 7139,   54,    3,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [   2, 1861, 1567,  517, 6541, 7761, 2120, 7178, 7139,   54,    3,    0,\n",
      "            0,    0,    0],\n",
      "        [   2, 3916, 6855, 4465, 6896, 1685, 2287, 7018, 6364, 6586,   54,    3,\n",
      "            0,    0,    0],\n",
      "        [   2, 3758, 5452, 7318, 5765, 7095, 2249, 6896, 1685, 3714, 7848, 6999,\n",
      "           54,    3,    0],\n",
      "        [   2,  517,    0, 6037, 5781, 7095, 1280, 3533, 6125, 3175, 6390, 6705,\n",
      "         5782,   54,    3],\n",
      "        [   2, 1575, 2046, 7088, 3057, 7848, 6364, 6586,   54,    3,    0,    0,\n",
      "            0,    0,    0]])\n",
      "torch.Size([8, 15, 768])\n",
      "tensor([[[ 0.0665,  0.1950,  0.3748,  ..., -0.3171, -0.3754, -0.4341],\n",
      "         [ 0.0514,  0.1262,  0.2550,  ..., -0.0333, -0.1232, -0.4420],\n",
      "         [-0.2128, -0.2556, -0.0950,  ...,  0.4923, -0.3260, -0.2222],\n",
      "         ...,\n",
      "         [ 0.0734, -0.5296,  0.9335,  ..., -0.1551, -0.1006, -0.0786],\n",
      "         [ 0.0881, -0.4885,  0.7998,  ..., -1.0189,  0.4391, -0.5981],\n",
      "         [ 0.0669,  0.1954,  0.3740,  ..., -0.3177, -0.3744, -0.4340]],\n",
      "\n",
      "        [[ 0.4588, -0.1187,  0.5098,  ..., -0.5527, -0.4542, -0.2333],\n",
      "         [-0.1186,  0.0578,  0.2030,  ..., -0.4930, -0.3352, -0.2218],\n",
      "         [ 0.3236, -0.1003,  0.7297,  ...,  0.0926, -0.0727, -0.6089],\n",
      "         ...,\n",
      "         [ 0.1511, -0.1424,  1.0129,  ..., -0.9324, -0.3891, -0.1333],\n",
      "         [ 0.0895, -0.1410,  1.0722,  ..., -0.9420, -0.3779, -0.1066],\n",
      "         [ 0.0590, -0.1716,  1.0471,  ..., -1.1013, -0.3005, -0.0892]],\n",
      "\n",
      "        [[ 0.3684,  0.1014,  0.4676,  ..., -0.5873, -0.5276, -0.1631],\n",
      "         [ 0.2058,  0.2376,  0.1568,  ..., -0.5111, -0.1901, -0.6947],\n",
      "         [ 0.0908, -0.2237,  0.4855,  ..., -0.2942, -0.0581, -0.1141],\n",
      "         ...,\n",
      "         [ 0.1650, -0.1503,  0.8941,  ..., -1.0253, -0.4328, -0.1855],\n",
      "         [ 0.1139, -0.1698,  0.9431,  ..., -1.0412, -0.4060, -0.1882],\n",
      "         [ 0.0944, -0.1698,  0.9439,  ..., -1.1267, -0.3314, -0.1667]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.4408, -0.0685,  0.3242,  ..., -0.3750,  0.0984, -0.3516],\n",
      "         [-0.0858, -0.0588, -0.1026,  ..., -0.2642,  0.1240, -0.1878],\n",
      "         [ 0.3708, -0.0086,  0.0264,  ..., -0.0191,  0.0518, -0.1437],\n",
      "         ...,\n",
      "         [ 0.0180, -0.4895,  0.4480,  ..., -0.9504,  0.6473, -0.4269],\n",
      "         [ 0.4387, -0.0669,  0.3219,  ..., -0.3747,  0.0979, -0.3497],\n",
      "         [ 0.1720, -0.5993,  0.8512,  ..., -1.0097,  0.0791, -0.5769]],\n",
      "\n",
      "        [[ 0.2877,  0.3696,  0.2362,  ..., -0.4323, -0.1673, -0.1147],\n",
      "         [-0.1880,  0.4194,  0.2230,  ..., -0.5990,  0.0220, -0.6324],\n",
      "         [ 0.3109, -0.0233,  0.6162,  ..., -0.4922, -0.3674,  0.0309],\n",
      "         ...,\n",
      "         [ 0.0592, -0.2300,  1.0351,  ..., -0.3538,  0.3457, -0.2324],\n",
      "         [ 0.1981, -0.3247,  0.7849,  ..., -1.0434,  0.4708, -0.6217],\n",
      "         [ 0.2878,  0.3693,  0.2350,  ..., -0.4322, -0.1668, -0.1150]],\n",
      "\n",
      "        [[ 0.2702,  0.2270,  0.5859,  ..., -0.4735, -0.5412, -0.2641],\n",
      "         [-0.0659, -0.2896,  0.9178,  ..., -0.3141, -0.7606, -0.0046],\n",
      "         [ 0.0169,  0.5999, -0.2115,  ..., -0.1437, -0.4686, -0.1951],\n",
      "         ...,\n",
      "         [ 0.1263, -0.1050,  1.0238,  ..., -0.9249, -0.3792, -0.1291],\n",
      "         [ 0.0534, -0.0857,  1.0962,  ..., -0.9047, -0.3617, -0.0651],\n",
      "         [ 0.0387, -0.1338,  1.0693,  ..., -1.0533, -0.2764, -0.0499]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from kobert_transformers import get_tokenizer\n",
    "import torch\n",
    "\n",
    "# Define your model\n",
    "model = distilbert_model  # 모델 선언\n",
    "tokenizer = get_tokenizer()  # 토크나이저 선언\n",
    "\n",
    "sentences = [\n",
    "    \"[CLS] 여기에 칼럼에서 뽑아낸 자연어를 넣으세요. [SEP]\",\n",
    "    \"[CLS] 다른 자연어 문장을 추가하세요. [SEP]\",\n",
    "    \"[CLS] 이 문장은 테스트용입니다. [SEP]\",\n",
    "    \"[CLS] 또 다른 샘플 문장입니다. [SEP]\",\n",
    "    \"[CLS] 자연어 처리에 대해 배워보세요. [SEP]\",\n",
    "    \"[CLS] 인공지능의 발전에 대해 이야기해요. [SEP]\",\n",
    "    \"[CLS] 딥러닝의 기본 원리를 알아봅시다. [SEP]\",\n",
    "    \"[CLS] 다양한 모델을 실험해보세요. [SEP]\"\n",
    "]#sentences에 해당하는 문장 리스트는 우리가 csv에서 뽑아올 리스트 이건 샘플\n",
    "#각 문장은 앞에 [CLS], 뒤에 [SEP]이 꼭 있어야한다.(이것도 전처리하면서 넣어볼것)\n",
    "\n",
    "# 각 문장을 토큰화, id화\n",
    "tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "tokenid_list = [tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_sentences]\n",
    "\n",
    "# sequence를 같은 길이로 padding\n",
    "max_length = max(len(ids) for ids in tokenid_list)\n",
    "padded_tokenid_list = [ids + [0] * (max_length - len(ids)) for ids in tokenid_list]\n",
    "\n",
    "# Longtensor로 dtype변경\n",
    "tokenid_tensor = torch.LongTensor(padded_tokenid_list)\n",
    "\n",
    "# Print the tensor\n",
    "print(tokenid_tensor)\n",
    "\n",
    "# 추론\n",
    "tensor_output = model(tokenid_tensor)  # 모델에 위의 전처리 과정을 거친 자연어를 input으로 넣는다\n",
    "print(tensor_output[0].shape)  # Output shape\n",
    "print(tensor_output[0])  # Output 확인\n",
    "\n",
    "# 출력은 (batchsize, 토큰id의 길이, 768). 여기서 768은 임베딩 공간의 크기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 코드를 적절히 변경해서 csv에서 불러온 자연어를 넣어서 tensor를 output으로 뽑고 저장하는 과정을 시도해보는것을 목표로 코드를 짜시면 됩니다.\n",
    "\n",
    "한번에 몇개의 문장리스트를 넣을지는 자유롭게 하시면 되지만 메모리 문제로 일정 크기이상의 문장리스트를 넣으면 메모리 문제로 중단될 수 있으니 32이하로 시도해보세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nof",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
