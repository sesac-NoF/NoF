{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 진형님 판시사항_명사_형태소_ngram 참조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 데이터 처리시 알아야 할 7가지 \n",
    "\n",
    "\n",
    "inplace=True \n",
    "\n",
    "memory_usage=\"deep\"\n",
    "\n",
    "%timeit pd.read_csv(\"\")\n",
    "\n",
    "pd.read_csv(\"\", usecols=[])\n",
    "\n",
    ".dtype\n",
    "\n",
    ".memory_usage()\n",
    "\n",
    ".shape[0]\n",
    "\n",
    ".nunique()\n",
    "\n",
    ".astype(\"\")\n",
    "\n",
    "astype(\"Sparse[float32]\")\n",
    "\n",
    "data = pd.read_csv(\"\", usecols=[], dtype={\"key\": np.int16, \"key2\": \"category\"})\n",
    "\n",
    "for chunk in pd.read_csv(\"\", chunksize=5000):\n",
    "\n",
    ".groupby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 로드_판례본문"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "저장된 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 한글 깨짐 방지\n",
    "plt.rcParams['font.family'] = \"Malgun Gothic\"\n",
    "\n",
    "# 마이너스 깨짐 방지\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일은 전처리 완료된 판례본문 / 판례이유 분리하여 열로 저장한 것\n",
    "df = pd.read_csv(r'C:\\Users\\user\\Desktop\\프로젝트 기획서\\NoF\\data\\df_labor_real_cleaned2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5016 entries, 0 to 5015\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    5016 non-null   int64 \n",
      " 1   판례정보일련번호      5016 non-null   int64 \n",
      " 2   사건번호          5016 non-null   object\n",
      " 3   선고            5016 non-null   object\n",
      " 4   판결유형          5016 non-null   object\n",
      " 5   판시사항          3725 non-null   object\n",
      " 6   판결요지          3054 non-null   object\n",
      " 7   참조조문          3724 non-null   object\n",
      " 8   참조판례          2450 non-null   object\n",
      " 9   판례내용          5016 non-null   object\n",
      " 10  판례내용_상단       5016 non-null   object\n",
      " 11  판례내용_이유       5012 non-null   object\n",
      " 12  판례내용_이유(전처리)  5016 non-null   object\n",
      " 13  판례내용_제거(불용어)  5016 non-null   object\n",
      "dtypes: int64(2), object(12)\n",
      "memory usage: 548.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5016 entries, 0 to 5015\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    5016 non-null   int64 \n",
      " 1   판례정보일련번호      5016 non-null   int64 \n",
      " 2   사건번호          5016 non-null   object\n",
      " 3   선고            5016 non-null   object\n",
      " 4   판결유형          5016 non-null   object\n",
      " 5   판시사항          3725 non-null   object\n",
      " 6   판결요지          3054 non-null   object\n",
      " 7   참조조문          3724 non-null   object\n",
      " 8   참조판례          2450 non-null   object\n",
      " 9   판례내용          5016 non-null   object\n",
      " 10  판례내용_상단       5016 non-null   object\n",
      " 11  판례내용_이유       5012 non-null   object\n",
      " 12  판례내용_이유(전처리)  5016 non-null   object\n",
      " 13  판례내용_제거(불용어)  5016 non-null   object\n",
      "dtypes: int64(2), object(12)\n",
      "memory usage: 232.4 MB\n"
     ]
    }
   ],
   "source": [
    "#메모리 할당량 계산\n",
    "\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.56 s ± 34.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "#csv 파일을 로드하는 런타임 계산\n",
    "\n",
    "%timeit pd.read_csv(r'C:\\Users\\user\\Desktop\\프로젝트 기획서\\NoF\\data\\df_labor_real_cleaned2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레코드 수: 5016\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'da_labor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 범주형 데이터를 나타내는 열의 데이터 유형 변경\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 데이터를 나타내는 열의 데이터 유형 변경\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m레코드 수:\u001b[39m\u001b[38;5;124m\"\u001b[39m, df_labor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m고유한 열값의 수:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mda_labor\u001b[49m\u001b[38;5;241m.\u001b[39m판시사항\u001b[38;5;241m.\u001b[39mnunique())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'da_labor' is not defined"
     ]
    }
   ],
   "source": [
    "# 범주형 데이터를 나타내는 열의 데이터 유형 변경\n",
    "# 데이터를 나타내는 열의 데이터 유형 변경\n",
    "\n",
    "print(\"레코드 수:\", df_labor.shape[0])\n",
    "print(\"고유한 열값의 수:\", df_labor.판시사항.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "print(df_labor.판례내용.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "불용어 제거하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#한국어 처리를 의한 라이브러리 사용 \n",
    "# Counter: 데이터를 카운팅하여 빈도수를 계산하기 위한 모듈, 주로 단어 빈도 계산에 사용\n",
    "from collections import Counter\n",
    "\n",
    "# konlpy의 Okt: 한국어 형태소 분석기. 텍스트를 형태소 단위로 분리하거나 품사 태깅에 사용\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# CountVectorizer: 문서에서 단어 빈도 수를 기반으로 피처 벡터를 생성하는 사이킷런의 도구\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re  # 정규 표현식 모듈\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 불용어 파일 로드\n",
    "stopwords_file_path = r'C:\\Users\\user\\Desktop\\프로젝트 기획서\\NoF\\data\\sorted_words.txt'\n",
    "stopwords_pick = ['제', '호로', '등', '것', '항', '의', '금', '검사', '판결', '거나', '유1', \n",
    "                  '상고이유', '기초', '사실', '기초사실', '사고', '피고', '원고', '피고인', \n",
    "                  '사건', '담당', '변호사', '변호인', '변론', '종결', '소외']\n",
    "pattern = r'제\\d+|\\d호증|\\d호'  # 정규 표현식 패턴\n",
    "\n",
    "# 불용어 파일을 읽어 불용어 목록 생성\n",
    "with open(stopwords_file_path, 'r', encoding='utf-8') as f:\n",
    "    stopwords = set(f.read().splitlines())\n",
    "\n",
    "# 불용어 제거 함수 (리스트 상태로 처리)\n",
    "def remove_stopwords(content):\n",
    "    # Okt 객체를 생성하여 형태소 분석\n",
    "    okt = Okt()\n",
    "    \n",
    "    # 형태소 분석을 통해 단어 리스트 생성\n",
    "    tokens = okt.morphs(content)  # 형태소 단위로 분리\n",
    "    filtered_words = [\n",
    "        word for word in tokens \n",
    "        if len(word) > 1 \n",
    "        and word not in stopwords \n",
    "        and not word.isdigit()  # 숫자인 경우 제외\n",
    "        and word not in stopwords_pick \n",
    "        and not re.match(pattern, word)\n",
    "    ]\n",
    "    return ' '.join(filtered_words)  # 문자열로 반환\n",
    "\n",
    "# 불용어 제거 수행 (판례내용 열의 내용을 처리)\n",
    "df_labor['판례내용_제거(불용어)'] = df_labor['판례내용'].apply(remove_stopwords)\n",
    "\n",
    "# 결과 확인\n",
    "print(df_labor['판례내용_제거(불용어)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "판례내용 길이분포 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분포"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1 표준할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labor = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labor['판례내용_제거(불용어)'] = df_labor['판례내용_제거(불용어)'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = df_labor['판례내용_제거(불용어)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 5016 entries, 0 to 5015\n",
      "Series name: 판례내용_제거(불용어)\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "5016 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 39.3+ KB\n",
      "None\n",
      "0    주식회사 ○○ 김소영 피상 중앙 노동 위원장 법무 법인 지향 김진 법무 법인 서희원...\n",
      "1    피상 조창 ○○○ 주식회사 김원정 부산 고법 창원 비용 부담 한다 한다 판시와 협력...\n",
      "2    피상 명단 방광 ○○○ 주식회사 김원정 부산 고법 창원 비용 부담 한다 지난 보충 ...\n",
      "3    피상 법무 법인 금강 ○○○ 주식회사 김원정 주식회사 △△△ 법무 법인 유한 태평양...\n",
      "4    법무 법인 창조 피상 대한민국 법무 법인 유한 세종 김주안 고법 연차 휴가 수당 주...\n",
      "Name: 판례내용_제거(불용어), dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data_list.info())\n",
    "print(data_list.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "메모리 사용량 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 5016 entries, 0 to 5015\n",
      "Series name: 판례내용_제거(불용어)\n",
      "Non-Null Count  Dtype \n",
      "--------------  ----- \n",
      "5016 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 33.5 MB\n"
     ]
    }
   ],
   "source": [
    "data_list.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<<<<메모리 줄이는 작업 시작 >>>>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명사추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#명사추출이랑 같음\n",
    "\n",
    "\n",
    "okt = Okt()\n",
    "for text in data_list:\n",
    "    nouns = okt.nouns(text)\n",
    "    docs_list4.append(' '.join(nouns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n-gram 원본"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 변경 전 메모리 부족으로 오류 발생 \n",
    "### 최대 단위 수 지정\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#1-gram\n",
    "vect = CountVectorizer() \n",
    "dtm = vect.fit_transform(docs_list4).toarray()\n",
    "vocab = vect.get_feature_names_out()\n",
    "df_1gram = pd.DataFrame(dtm, columns=vocab)\n",
    "\n",
    "#2-gram\n",
    "vect_2gram = CountVectorizer(ngram_range=(2, 2))\n",
    "dtm_2gram = vect_2gram.fit_transform(docs_list4).toarray()\n",
    "vocab_2gram = vect_2gram.get_feature_names_out()\n",
    "df_2gram = pd.DataFrame(dtm_2gram, columns=vocab_2gram)\n",
    "\n",
    "#3-gram \n",
    "vect_3gram = CountVectorizer(ngram_range=(3, 3))\n",
    "dtm_3gram = vect_3gram.fit_transform(docs_list4).toarray()\n",
    "vocab_3gram = vect_3gram.get_feature_names_out()\n",
    "df_3gram = pd.DataFrame(dtm_3gram, columns=vocab_3gram)\n",
    "\n",
    "print(\"1-gram\")\n",
    "print(df_1gram)\n",
    "\n",
    "print(\"2-gram\")\n",
    "print(df_2gram)\n",
    "\n",
    "print(\"3-gram\")\n",
    "print(df_3gram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* n-gram 변경코드\n",
    "* chunk size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def process_in_chunks(docs_list4, ngram_range=(1, 1), max_features=None, chunk_size=10):\n",
    "    df_list = []  # 결과를 저장할 리스트\n",
    "    vect = CountVectorizer(ngram_range=ngram_range, max_features=max_features)\n",
    "    \n",
    "    # 데이터셋을 청크로 나누어 처리\n",
    "    for i in range(0, len(docs_list4), chunk_size):\n",
    "        chunk = docs_list4[i:i + chunk_size]\n",
    "        dtm_chunk = vect.fit_transform(chunk)  # 희소 행렬로 유지\n",
    "        vocab_chunk = vect.get_feature_names_out()\n",
    "        \n",
    "        # 희소 행렬을 데이터프레임으로 변환 (toarray()를 사용하지 않고 처리)\n",
    "        df_chunk = pd.DataFrame.sparse.from_spmatrix(dtm_chunk, columns=vocab_chunk)\n",
    "        df_list.append(df_chunk)\n",
    "    \n",
    "    # 모든 청크를 하나의 데이터프레임으로 결합\n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# 2-gram 처리 (단어 수 제한, 메모리 문제 방지)\n",
    "df_2gram = process_in_chunks(docs_list4, ngram_range=(2, 2))\n",
    "\n",
    "# 결과 출력\n",
    "print(\"2-gram\")\n",
    "print(df_2gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 시각화 함수\n",
    "def plot_top_n(df, title, n=None):\n",
    "    # 각 n-gram의 빈도 계산\n",
    "    freq = df.sum().sort_values(ascending=False).head(n)\n",
    "    \n",
    "    # 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(freq.index, freq.values, color='skyblue')\n",
    "    plt.xlabel('빈도')\n",
    "    plt.title(title)\n",
    "    plt.gca().invert_yaxis()  # 빈도순으로\n",
    "    plt.show()\n",
    "\n",
    "# 상위 20개 빈도 시각화\n",
    "plot_top_n(df_1gram, '1-gram 빈도',20)\n",
    "plot_top_n(df_2gram, '2-gram 빈도',20)\n",
    "plot_top_n(df_3gram, '3-gram 빈도',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from konlpy.tag import Okt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "\n",
    "# 불용어 설정\n",
    "stop_words = ['이', '그', '저']\n",
    "\n",
    "# Okt 객체 생성\n",
    "okt = Okt()\n",
    "\n",
    "# 데이터 청크 분할\n",
    "def split_data(data, chunk_size=250):\n",
    "    return np.array_split(data, len(data) // chunk_size)\n",
    "\n",
    "# 명사 필터링 및 n-gram 생성 함수\n",
    "def process_ngrams(chunk):\n",
    "    processed_texts = []\n",
    "    for text in chunk['text_column']:\n",
    "        nouns = okt.nouns(text)\n",
    "        filtered_nouns = ' '.join([noun for noun in nouns if noun not in stop_words])\n",
    "        processed_texts.append(filtered_nouns)\n",
    "    chunk['processed_text'] = processed_texts\n",
    "    return chunk\n",
    "\n",
    "# 데이터 로드 및 분할\n",
    "data = pd.DataFrame({'text_column': [\"여기에 분석할 텍스트를 넣으세요.\"] * 5016})\n",
    "data_chunks = split_data(data)\n",
    "\n",
    "# 병렬 처리 시작\n",
    "start_time = time.time()\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(process_ngrams, data_chunks))\n",
    "\n",
    "# 처리 결과 병합\n",
    "processed_data = pd.concat(results, ignore_index=True)\n",
    "\n",
    "# 3-gram 벡터화\n",
    "vectorizer = CountVectorizer(ngram_range=(3, 3), max_df=0.9)\n",
    "X = vectorizer.fit_transform(processed_data['processed_text'])\n",
    "\n",
    "# 완료 시간 출력\n",
    "total_time = time.time() - start_time\n",
    "print(f\"전체 처리 완료, 총 경과 시간: {total_time:.2f}초\")\n",
    "\n",
    "# n-gram 목록 저장\n",
    "ngram_features = vectorizer.get_feature_names_out()\n",
    "pd.DataFrame(ngram_features, columns=['3-gram']).to_csv(\"ngram_features.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
