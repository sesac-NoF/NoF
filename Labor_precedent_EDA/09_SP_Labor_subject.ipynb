{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. '근로'에 대한 판례내용에서 가장 자주 나타나는 소송 주제 찾기\n",
    "2. 다음의 열에 있는 값들과 상관관계 알고 싶어 \n",
    "    -열 이름 : 법원명, 사건종류명, 재판당사자\n",
    "3. '근로'에 대한 판례에서 가장 자주 쓰이는 참조판례, 참조조문\n",
    "   -열 이름 : 참조조문, 참조판례"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. '근로'에 대한 판례내용에서 가장 자주 나타나는 소송 주제 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. '근로'에 대한 판례내용에서 가장 자주 나타나는 소송 주제 찾기\n",
    "def find_frequent_cases(data, keyword='근로', num_terms=10):\n",
    "    filtered_data = data[data['판례내용'].str.contains(keyword)]\n",
    "    all_terms = ' '.join(filtered_data['processed'])\n",
    "    term_counts = Counter(all_terms.split())\n",
    "    most_common_terms = term_counts.most_common(num_terms)\n",
    "    \n",
    "    terms, counts = zip(*most_common_terms)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(counts), y=list(terms))\n",
    "    plt.title(f'Top {keyword} Related Legal Terms Frequency')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Terms')\n",
    "    plt.show()\n",
    "\n",
    "find_frequent_cases(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 법원명, 사건종류명, 재판당사자와 '근로'에 대한 판례 내용의 상관관계 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. 법원명, 사건종류명, 재판당사자와 상관관계 분석\n",
    "def analyze_correlation(data, keyword='근로'):\n",
    "    filtered_data = data[data['판례내용'].str.contains(keyword)]\n",
    "    \n",
    "    # 법원명, 사건종류명, 재판당사자와 '근로' 관련 판례 내용의 빈도 수 계산\n",
    "    correlation_data = filtered_data[['법원명', '사건종류명', '재판당사자']]\n",
    "    \n",
    "    # 각 열의 고유값 수 카운트\n",
    "    correlation_counts = correlation_data.apply(lambda x: x.value_counts()).fillna(0)\n",
    "    \n",
    "    # 상관관계 시각화\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(correlation_counts, annot=True, cmap='coolwarm')\n",
    "    plt.title(f'Correlation Analysis of Court, Case Type, and Parties related to {keyword}')\n",
    "    plt.show()\n",
    "\n",
    "analyze_correlation(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. '근로'에 대한 판례에서 가장 자주 쓰이는 참조판례, 참조조문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. '근로'에 대한 판례에서 가장 자주 쓰이는 참조판례, 참조조문\n",
    "def find_frequent_references(data, keyword='근로', num_terms=10):\n",
    "    filtered_data = data[data['판례내용'].str.contains(keyword)]\n",
    "    \n",
    "    # 참조판례와 참조조문의 빈도 계산\n",
    "    reference_cases = filtered_data['참조판례'].str.split(',').explode().str.strip().dropna()\n",
    "    reference_articles = filtered_data['참조조문'].str.split(',').explode().str.strip().dropna()\n",
    "    \n",
    "    case_counts = Counter(reference_cases)\n",
    "    article_counts = Counter(reference_articles)\n",
    "    \n",
    "    # 참조판례 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    most_common_cases = case_counts.most_common(num_terms)\n",
    "    cases, case_frequencies = zip(*most_common_cases)\n",
    "    sns.barplot(x=list(case_frequencies), y=list(cases))\n",
    "    plt.title('Top Referenced Cases')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Cases')\n",
    "\n",
    "    # 참조조문 시각화\n",
    "    plt.subplot(1, 2, 2)\n",
    "    most_common_articles = article_counts.most_common(num_terms)\n",
    "    articles, article_frequencies = zip(*most_common_articles)\n",
    "    sns.barplot(x=list(article_frequencies), y=list(articles))\n",
    "    plt.title('Top Referenced Articles')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Articles')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "find_frequent_references(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "재판당사자 열에서 단어간의 앞뒤 상관관계를 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 준비: 재판당사자 열에서 데이터를 로드합니다.\n",
    "단어 쌍 생성: 각 재판당사자 항목에서 단어 간의 쌍을 생성합니다.\n",
    "빈도 분석: 생성된 단어 쌍의 빈도를 계산합니다.\n",
    "상관관계 시각화: 빈도수에 기반하여 단어 쌍의 상관관계를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "\n",
    "# 1. 데이터 로드\n",
    "data = pd.read_csv('service.csv', encoding='utf-8')\n",
    "\n",
    "# 2. 재판당사자 열에서 단어 간의 앞뒤 상관관계 분석\n",
    "def analyze_parties_relationship(data):\n",
    "    # 재판당사자 열에서 단어 쌍 생성\n",
    "    word_pairs = []\n",
    "    for parties in data['재판당사자'].dropna():\n",
    "        words = parties.split()  # 공백을 기준으로 단어 분리\n",
    "        # 단어 간의 조합 생성 (앞뒤 관계)\n",
    "        word_pairs.extend(combinations(words, 2))\n",
    "\n",
    "    # 단어 쌍의 빈도 계산\n",
    "    pair_counts = Counter(word_pairs)\n",
    "    \n",
    "    # 상위 10개의 단어 쌍 추출\n",
    "    most_common_pairs = pair_counts.most_common(10)\n",
    "    pairs, counts = zip(*most_common_pairs)\n",
    "\n",
    "    # 단어 쌍 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=list(counts), y=[' '.join(pair) for pair in pairs])\n",
    "    plt.title('Top 10 Word Pairs in Parties')\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Word Pairs')\n",
    "    plt.show()\n",
    "\n",
    "analyze_parties_relationship(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "법원명에 따른 유사성 혹은 차별성 분석\n",
    " -법원별 사건명에서 주제 도출 판례내용_이유에서 유의미한 값 도출\n",
    " -도출된 주제에서 유사성 혹은 차별성 분석 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "법원별 사건명 추출: 법원별로 사건명을 그룹화하여 판례내용_이유와 함께 사용합니다.\n",
    "주제 도출: LDA(잠재 디리클레 할당) 모델을 사용하여 법원별로 주제를 도출합니다.\n",
    "유사성 및 차별성 분석: 도출된 주제 간의 유사성 또는 차별성을 분석합니다. 이를 위해 코사인 유사도(cosine similarity)를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 준비 및 주제 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1. 데이터 로드\n",
    "data = pd.read_csv('service.csv', encoding='utf-8')\n",
    "\n",
    "# 2. 법원별 사건명 및 판례내용_이유 추출\n",
    "court_cases = data.groupby('법원명')['사건명', '판례내용_이유'].apply(lambda x: ' '.join(x['판례내용_이유'])).reset_index()\n",
    "court_cases.columns = ['법원명', '판례내용_이유']\n",
    "\n",
    "# 3. LDA 모델을 통해 주제 도출\n",
    "def extract_topics(data, n_topics=5, n_top_words=10):\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    doc_term_matrix = vectorizer.fit_transform(data['판례내용_이유'])\n",
    "\n",
    "    lda_model = LDA(n_components=n_topics, random_state=42)\n",
    "    lda_model.fit(doc_term_matrix)\n",
    "\n",
    "    # 각 주제에 대한 단어 추출\n",
    "    topics = []\n",
    "    for idx, topic in enumerate(lda_model.components_):\n",
    "        topic_words = [vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-n_top_words:]]\n",
    "        topics.append(f'Topic {idx + 1}: ' + ', '.join(topic_words))\n",
    "    \n",
    "    return topics\n",
    "\n",
    "topics = extract_topics(court_cases)\n",
    "for topic in topics:\n",
    "    print(topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 유사성 또는 차별성 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. 주제 간의 유사성 분석\n",
    "def analyze_similarity(data, topics):\n",
    "    topic_vectors = []\n",
    "    vectorizer = CountVectorizer()\n",
    "    for topic in topics:\n",
    "        topic_words = topic.replace('Topic', '').split(': ')[1].split(', ')\n",
    "        topic_vector = vectorizer.fit_transform([' '.join(topic_words)]).toarray()\n",
    "        topic_vectors.append(topic_vector.flatten())\n",
    "\n",
    "    # 코사인 유사도 계산\n",
    "    similarity_matrix = cosine_similarity(topic_vectors)\n",
    "\n",
    "    # 유사성 시각화\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm', xticklabels=[f'Topic {i + 1}' for i in range(len(topics))],\n",
    "                yticklabels=[f'Topic {i + 1}' for i in range(len(topics))])\n",
    "    plt.title('Topic Similarity Matrix')\n",
    "    plt.show()\n",
    "\n",
    "analyze_similarity(court_cases, topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 재판당사자별 빈도수 분석\n",
    "- 사건별 재판당사자 수에 따른 특징 분석\n",
    "- 재판당사자와 사건내용에 따른 상관관계 도출\n",
    "- 네트워크 분석을 위한 인사이트 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 재판당사자별 빈도수 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('service.csv', encoding='utf-8')\n",
    "\n",
    "# 재판당사자 열에서 단어별 빈도수 계산\n",
    "def analyze_parties_frequency(data):\n",
    "    # 재판당사자 열에서 모든 단어 추출\n",
    "    all_parties = ' '.join(data['재판당사자'].dropna()).split()\n",
    "    \n",
    "    # 빈도수 계산\n",
    "    party_counts = Counter(all_parties)\n",
    "    \n",
    "    # 상위 10개의 재판당사자 추출\n",
    "    most_common_parties = party_counts.most_common(10)\n",
    "    parties, counts = zip(*most_common_parties)\n",
    "\n",
    "    # 빈도수 시각화\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(parties, counts)\n",
    "    plt.title('Top 10 Most Frequent Parties')\n",
    "    plt.xlabel('Parties')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "analyze_parties_frequency(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 사건별 재판당사자 수에 따른 특징 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 사건별 재판당사자 수 분석\n",
    "def analyze_case_party_count(data):\n",
    "    # 각 사건별 재판당사자 수 계산\n",
    "    data['party_count'] = data['재판당사자'].apply(lambda x: len(str(x).split()))\n",
    "    \n",
    "    # 사건별 특징 분석\n",
    "    case_party_counts = data.groupby('사건명')['party_count'].mean().reset_index()\n",
    "    \n",
    "    # 상위 10개의 사건 특징 분석\n",
    "    top_cases = case_party_counts.nlargest(10, 'party_count')\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(top_cases['사건명'], top_cases['party_count'])\n",
    "    plt.title('Top 10 Cases by Average Number of Parties')\n",
    "    plt.xlabel('Average Number of Parties')\n",
    "    plt.ylabel('Cases')\n",
    "    plt.show()\n",
    "\n",
    "analyze_case_party_count(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 재판당사자와 사건내용에 따른 상관관계 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# 재판당사자와 사건내용 상관관계 분석\n",
    "def analyze_parties_content_correlation(data):\n",
    "    # 사건내용과 재판당사자 벡터화\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_parties = vectorizer.fit_transform(data['재판당사자'].dropna())\n",
    "    X_content = vectorizer.fit_transform(data['판례내용'].dropna())\n",
    "\n",
    "    # Jaccard 유사도 계산\n",
    "    correlation_matrix = []\n",
    "    for i in range(X_parties.shape[0]):\n",
    "        row_correlation = []\n",
    "        for j in range(X_content.shape[0]):\n",
    "            jaccard = jaccard_score(X_parties[i].toarray(), X_content[j].toarray())\n",
    "            row_correlation.append(jaccard)\n",
    "        correlation_matrix.append(row_correlation)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(correlation_matrix, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
    "    plt.title('Correlation between Parties and Case Contents')\n",
    "    plt.xlabel('Case Contents')\n",
    "    plt.ylabel('Parties')\n",
    "    plt.show()\n",
    "\n",
    "analyze_parties_content_correlation(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 네트워크 분석을 위한 인사이트 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# 네트워크 분석을 위한 인사이트 도출\n",
    "def network_analysis(data):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # 재판당사자와 사건명 간의 관계 추가\n",
    "    for idx, row in data.iterrows():\n",
    "        parties = str(row['재판당사자']).split()\n",
    "        case_name = row['사건명']\n",
    "        \n",
    "        # 재판당사자와 사건명 간의 엣지 생성\n",
    "        for party in parties:\n",
    "            G.add_edge(party, case_name)\n",
    "\n",
    "    # 네트워크 시각화\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    pos = nx.spring_layout(G)\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=100, node_color='blue', alpha=0.5)\n",
    "    nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "    plt.title('Network of Parties and Cases')\n",
    "    plt.show()\n",
    "\n",
    "network_analysis(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "판사, 변호사와 판례본문의 항목간의 상관관계 분석\n",
    "-항목명 : 법원명, 사건종류명, 판결유형, 판시사항, 판결요지, 참조조문, 참조판례"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 준비 및 상관관계 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 분석 설명\n",
    "데이터 준비:\n",
    "\n",
    "필요한 열을 추출하고 결측치를 제거합니다.\n",
    "각 항목의 길이를 계산하여, 길이 간의 상관관계를 분석합니다. 이는 항목의 내용이 얼마나 길게 서술되는지를 확인하는 데 유용합니다.\n",
    "상관관계 분석:\n",
    "\n",
    "corr() 함수를 사용하여 각 열 간의 상관관계를 계산합니다.\n",
    "시각화:\n",
    "\n",
    "seaborn 라이브러리를 사용하여 상관관계 히트맵을 생성하고, 이를 통해 항목 간의 관계를 직관적으로 이해할 수 있도록 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('service.csv', encoding='utf-8')\n",
    "\n",
    "# 필요한 항목 추출\n",
    "required_columns = ['법원명', '사건종류명', '판결유형', '판시사항', '판결요지', '참조조문', '참조판례']\n",
    "data_subset = data[required_columns]\n",
    "\n",
    "# 결측치 제거\n",
    "data_subset.dropna(inplace=True)\n",
    "\n",
    "# 각 항목의 길이를 계산하여 상관관계 분석\n",
    "data_subset['법원명_len'] = data_subset['법원명'].str.len()\n",
    "data_subset['사건종류명_len'] = data_subset['사건종류명'].str.len()\n",
    "data_subset['판결유형_len'] = data_subset['판결유형'].str.len()\n",
    "data_subset['판시사항_len'] = data_subset['판시사항'].str.len()\n",
    "data_subset['판결요지_len'] = data_subset['판결요지'].str.len()\n",
    "data_subset['참조조문_len'] = data_subset['참조조문'].str.len()\n",
    "data_subset['참조판례_len'] = data_subset['참조판례'].str.len()\n",
    "\n",
    "# 상관관계 계산\n",
    "correlation_matrix = data_subset.corr()\n",
    "\n",
    "# 상관관계 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation between Judicial Attributes and Case Text')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'근로'에 대한 판례내용에서 등장하는 중심 단어와 그 주변 단어들의 네트워크 분석을 수행하려면 다음과 같은 단계로 진행할 수 있습니다:\n",
    "\n",
    "데이터 필터링: '근로'와 관련된 판례 내용을 필터링합니다.\n",
    "토큰화: 필터링된 판례 내용을 토큰화하여 단어를 추출합니다.\n",
    "네트워크 구축: 중심 단어와 주변 단어 간의 관계를 기반으로 네트워크를 구축합니다.\n",
    "네트워크 시각화: 네트워크를 시각화하여 중심 단어와 주변 단어 간의 관계를 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 필터링 및 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('service.csv', encoding='utf-8')\n",
    "\n",
    "# '근로' 관련 판례 내용 필터링\n",
    "labor_cases = data[data['판례내용'].str.contains('근로', na=False)]\n",
    "\n",
    "# 판례 내용에서 텍스트 추출 및 전처리\n",
    "def preprocess_text(text):\n",
    "    # 소문자 변환 및 특수문자 제거\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)  # 한국어 및 공백만 남김\n",
    "    return text\n",
    "\n",
    "# 모든 판례 내용을 전처리\n",
    "labor_cases['processed_text'] = labor_cases['판례내용'].apply(preprocess_text)\n",
    "\n",
    "# 토큰화\n",
    "all_words = ' '.join(labor_cases['processed_text']).split()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 중심 단어와 주변 단어 간의 네트워크 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 중심 단어 설정\n",
    "central_word = '근로'\n",
    "\n",
    "# 주변 단어 추출\n",
    "def get_surrounding_words(words, central_word, window_size=2):\n",
    "    central_indices = [i for i, word in enumerate(words) if word == central_word]\n",
    "    surrounding_words = []\n",
    "\n",
    "    for index in central_indices:\n",
    "        start = max(0, index - window_size)\n",
    "        end = min(len(words), index + window_size + 1)\n",
    "        surrounding_words.extend(words[start:index] + words[index + 1:end])\n",
    "    \n",
    "    return surrounding_words\n",
    "\n",
    "# 주변 단어 추출\n",
    "surrounding_words = get_surrounding_words(all_words, central_word)\n",
    "\n",
    "# 주변 단어 빈도수 계산\n",
    "surrounding_counts = Counter(surrounding_words)\n",
    "\n",
    "# 네트워크 구축\n",
    "G = nx.Graph()\n",
    "\n",
    "# 중심 단어와 주변 단어 간의 관계 추가\n",
    "for word, count in surrounding_counts.items():\n",
    "    G.add_edge(central_word, word, weight=count)\n",
    "\n",
    "# 노드 크기 조정\n",
    "node_sizes = [G.degree(n) * 100 for n in G.nodes()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 네트워크 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 네트워크 시각화\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='skyblue', alpha=0.7)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5)\n",
    "nx.draw_networkx_labels(G, pos, font_size=12)\n",
    "plt.title('Network Analysis of Central and Surrounding Words Related to \"근로\"')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "판례 내용에서 산업군이나 특징적인 주제를 도출하기 위해 특정 주제어들을 추출하고 분석하는 방법은 다음과 같은 단계로 진행할 수 있습니다:\n",
    "\n",
    "데이터 준비: 판례 내용을 로드하고 전처리합니다.\n",
    "주제어 추출: 텍스트에서 중요한 주제어를 추출합니다.\n",
    "주제어 분석: 주제어들의 빈도를 확인하고 분석합니다.\n",
    "주제어 클러스터링: 주제어를 클러스터링하여 산업군 또는 특징적인 주제를 도출합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('service.csv', encoding='utf-8')\n",
    "\n",
    "# 판례 내용 추출\n",
    "case_contents = data['판례내용'].dropna().tolist()\n",
    "\n",
    "# 판례 내용 전처리\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)  # 한국어 및 공백만 남김\n",
    "    return text\n",
    "\n",
    "# 전처리된 판례 내용\n",
    "processed_contents = [preprocess_text(content) for content in case_contents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 주제어 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 주제어 추출을 위한 CountVectorizer 설정\n",
    "vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
    "X = vectorizer.fit_transform(processed_contents)\n",
    "\n",
    "# 주제어 추출\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "word_counts = X.toarray().sum(axis=0)\n",
    "\n",
    "# 주제어와 그 빈도수\n",
    "topic_words = pd.DataFrame({'주제어': feature_names, '빈도수': word_counts})\n",
    "topic_words = topic_words[topic_words['빈도수'] > 0].sort_values(by='빈도수', ascending=False)\n",
    "\n",
    "print(topic_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 주제어 분석\n",
    "위의 코드에서 topic_words 데이터프레임은 주제어와 그 빈도수를 포함하고 있으며, 주제어의 빈도수를 출력합니다. 이 데이터프레임을 통해 가장 빈도가 높은 주제어를 확인할 수 있습니다.\n",
    "\n",
    "4. 주제어 클러스터링\n",
    "주제어 클러스터링을 위해 KMeans 알고리즘을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# KMeans 클러스터링\n",
    "num_clusters = 5  # 원하는 클러스터 수\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 클러스터 레이블\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# 클러스터링 결과 추가\n",
    "topic_words['클러스터'] = labels\n",
    "\n",
    "# 각 클러스터의 주제어 출력\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\n클러스터 {cluster}:\")\n",
    "    print(topic_words[topic_words['클러스터'] == cluster]['주제어'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans 클러스터링을 사용하여 판례 내용에서 도출된 주제어를 분석할 때, 여러 가지 인사이트를 얻을 수 있습니다. 아래는 KMeans 클러스터링을 통해 도출할 수 있는 주요 인사이트입니다:\n",
    "\n",
    "1. 산업군 및 주제 파악\n",
    "산업군 식별: 각 클러스터에 포함된 주제어를 분석하여 해당 클러스터가 어떤 산업군에 속하는지 파악할 수 있습니다. 예를 들어, \"근로\", \"임금\", \"계약\"과 같은 단어가 클러스터에 포함되면, 이는 노동법 관련 주제를 나타낼 수 있습니다.\n",
    "주제 구분: 판례가 다루는 다양한 주제를 명확히 구분할 수 있습니다. 예를 들어, 특정 클러스터가 법률적 분쟁에 관한 것이라면, 다른 클러스터는 규제나 정책과 관련된 내용을 포함할 수 있습니다.\n",
    "2. 특징적인 패턴 분석\n",
    "빈도수 및 경향 분석: 각 클러스터 내 주제어의 빈도를 확인함으로써 어떤 주제가 더 자주 다뤄지고 있는지를 알 수 있습니다. 이 정보는 법적 쟁점이나 산업 내 주요 이슈를 파악하는 데 유용합니다.\n",
    "변화 추세 탐지: 시간에 따른 주제어의 변화를 추적하여 법률이나 산업의 변화, 사회적 트렌드를 분석할 수 있습니다.\n",
    "3. 정책 및 전략적 인사이트\n",
    "법률적 대응 방안: 특정 주제가 클러스터링에서 두드러지게 나타나면, 해당 주제에 대한 법률적 대응 방안이나 정책 개선이 필요할 수 있습니다.\n",
    "리스크 관리: 특정 산업군에서 자주 발생하는 법적 쟁점이나 분쟁을 분석하여, 기업이나 법률 전문가가 리스크를 사전에 관리할 수 있는 전략을 마련할 수 있습니다.\n",
    "4. 법률 연구 및 학술적 기초\n",
    "연구 방향 제시: 각 클러스터의 주제어 분석을 통해 향후 연구가 필요한 법률 분야를 제안할 수 있습니다. 특히, 덜 다뤄진 주제나 이슈를 파악하여 심화 연구를 진행할 수 있습니다.\n",
    "법률 교육 커리큘럼 개발: 교육 기관에서는 클러스터링 결과를 바탕으로 법률 교육 커리큘럼을 개발하여, 학생들이 필요로 하는 주제를 더 잘 다룰 수 있도록 할 수 있습니다.\n",
    "5. 네트워크 분석 및 관계 도출\n",
    "관계 탐색: 클러스터 내 주제어 간의 관계를 분석하여, 특정 주제들이 서로 어떻게 연관되는지를 이해할 수 있습니다. 이를 통해 주제어 간의 네트워크를 시각화하여 더 깊은 인사이트를 제공할 수 있습니다.\n",
    "변호사 및 판사 관계: 클러스터링 결과를 통해 특정 산업군에서 자주 나타나는 변호사나 판사 간의 관계를 분석하여, 특정 분야에서의 영향력 있는 인물이나 그룹을 파악할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "판례 내용에서 연관된 다양한 주제를 클러스터링하기 위해 KMeans 알고리즘을 활용하는 프로세스는 다음과 같습니다. 이 프로세스는 데이터를 준비하고, 텍스트를 전처리한 후, 주제어를 추출하고 클러스터링을 수행하는 단계로 구성됩니다.\n",
    "\n",
    "아래의 단계별 코드를 통해 연관된 다양한 주제의 클러스터링을 도출할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('service.csv', encoding='utf-8')\n",
    "\n",
    "# 판례 내용 추출\n",
    "case_contents = data['판례내용'].dropna().tolist()\n",
    "\n",
    "# 판례 내용 전처리\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # 소문자 변환\n",
    "    text = re.sub(r'[^가-힣\\s]', '', text)  # 한국어 및 공백만 남김\n",
    "    return text\n",
    "\n",
    "# 전처리된 판례 내용\n",
    "processed_contents = [preprocess_text(content) for content in case_contents]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 주제어 추출 및 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 주제어 추출을 위한 CountVectorizer 설정\n",
    "vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "X = vectorizer.fit_transform(processed_contents)\n",
    "\n",
    "# 주제어 추출\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "word_counts = X.toarray().sum(axis=0)\n",
    "\n",
    "# 주제어와 그 빈도수\n",
    "topic_words = pd.DataFrame({'주제어': feature_names, '빈도수': word_counts})\n",
    "topic_words = topic_words[topic_words['빈도수'] > 0].sort_values(by='빈도수', ascending=False)\n",
    "\n",
    "print(\"주제어 빈도수:\")\n",
    "print(topic_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. KMeans 클러스터링 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# KMeans 클러스터링\n",
    "num_clusters = 5  # 원하는 클러스터 수\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# 클러스터 레이블\n",
    "labels = kmeans.labels_\n",
    "\n",
    "# 클러스터링 결과 추가\n",
    "topic_words['클러스터'] = labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 클러스터 결과 분석 및 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 각 클러스터의 주제어 출력\n",
    "for cluster in range(num_clusters):\n",
    "    print(f\"\\n클러스터 {cluster}:\")\n",
    "    print(topic_words[topic_words['클러스터'] == cluster]['주제어'].tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "도출된 주제의 핵심 키워드와 연관된 많이 사용하는 용어들을 출력하고, 이를 네트워크로 시각화하며, 또한 워드 클라우드로 표현하기 위한 단계는 다음과 같습니다.\n",
    "\n",
    "1. 데이터 준비 및 전처리\n",
    "이전 단계에서 판례 내용을 전처리하고 KMeans 클러스터링을 수행한 후, 각 클러스터의 키워드와 관련 용어를 추출해야 합니다. 아래 코드는 이를 수행하는 방법을 보여줍니다.\n",
    "\n",
    "2. 핵심 키워드 추출\n",
    "각 클러스터에서 핵심 키워드를 출력하고, 이를 기반으로 연관된 용어를 추출합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 각 클러스터의 주제어 출력 및 연관 단어 추출\n",
    "cluster_keywords = {}\n",
    "\n",
    "for cluster in range(num_clusters):\n",
    "    keywords = topic_words[topic_words['클러스터'] == cluster]['주제어'].tolist()\n",
    "    cluster_keywords[cluster] = keywords\n",
    "    print(f\"\\n클러스터 {cluster}의 핵심 키워드: {keywords}\")\n",
    "\n",
    "    # 연관 단어 추출\n",
    "    for keyword in keywords:\n",
    "        # 각 키워드와 관련된 용어 추출 (여기서는 빈도수 기준으로 정렬된 단어를 사용)\n",
    "        related_words = topic_words[topic_words['주제어'].str.contains(keyword)]['주제어'].tolist()\n",
    "        print(f\"'{keyword}'와 연관된 용어: {related_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 네트워크 시각화\n",
    "핵심 키워드와 연관된 용어를 네트워크로 시각화하기 위해 networkx 및 matplotlib 라이브러리를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 네트워크 그래프 생성\n",
    "G = nx.Graph()\n",
    "\n",
    "# 클러스터 내 키워드 및 연관 단어 간의 관계 추가\n",
    "for cluster, keywords in cluster_keywords.items():\n",
    "    for keyword in keywords:\n",
    "        G.add_node(keyword)  # 키워드 노드 추가\n",
    "        for related in topic_words[topic_words['주제어'].str.contains(keyword)]['주제어']:\n",
    "            G.add_node(related)  # 연관 단어 노드 추가\n",
    "            G.add_edge(keyword, related)  # 간선 추가\n",
    "\n",
    "# 그래프 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_size=2000, node_color='lightblue', font_size=10, font_color='black', font_weight='bold', edge_color='gray')\n",
    "plt.title('주요 키워드와 연관된 용어 네트워크', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 워드 클라우드 시각화\n",
    "워드 클라우드를 생성하기 위해 WordCloud 라이브러리를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# 전체 텍스트로 워드 클라우드 생성\n",
    "all_keywords = ' '.join(topic_words['주제어'].tolist())\n",
    "wordcloud = WordCloud(font_path='path_to_korean_font.ttf', width=800, height=400, background_color='white').generate(all_keywords)\n",
    "\n",
    "# 워드 클라우드 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('주요 키워드 워드 클라우드', fontsize=16)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사건번호와 선고일자에 따른 참조판례 및 참조조문의 변화를 시계열 분석하고, 이를 시각화하는 방법에 대한 설명을 아래에 제공합니다. 이 과정에서는 pandas와 matplotlib를 사용하여 데이터를 처리하고 시각화합니다.\n",
    "\n",
    "1. 데이터 준비 및 전처리\n",
    "우선, 필요한 데이터를 로드하고 전처리하는 과정을 거칩니다. 여기서는 사건번호, 선고일자, 참조판례, 참조조문이 포함된 데이터프레임을 가정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 데이터 로드\n",
    "data = pd.read_csv('service.csv', encoding='utf-8')\n",
    "\n",
    "# 필요한 열 추출 및 결측치 제거\n",
    "data = data[['사건번호', '선고일자', '참조판례', '참조조문']].dropna()\n",
    "\n",
    "# 선고일자 데이터 형식 변환\n",
    "data['선고일자'] = pd.to_datetime(data['선고일자'], errors='coerce')\n",
    "\n",
    "# 데이터 확인\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 참조판례 및 참조조문 빈도수 집계\n",
    "사건번호와 선고일자에 따른 참조판례 및 참조조문의 빈도수를 집계합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 선고일자에 따른 참조판례 및 참조조문 빈도수 집계\n",
    "reference_cases = data.groupby(['선고일자', '참조판례']).size().reset_index(name='빈도수')\n",
    "reference_statutes = data.groupby(['선고일자', '참조조문']).size().reset_index(name='빈도수')\n",
    "\n",
    "# 결과 확인\n",
    "print(reference_cases.head())\n",
    "print(reference_statutes.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 시계열 분석\n",
    "시계열 데이터를 분석하기 위해 월별 혹은 연도별로 집계하여 시계열 변화를 확인합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 월별 집계\n",
    "reference_cases['월'] = reference_cases['선고일자'].dt.to_period('M')\n",
    "reference_statutes['월'] = reference_statutes['선고일자'].dt.to_period('M')\n",
    "\n",
    "# 월별 참조판례 빈도수 집계\n",
    "monthly_cases = reference_cases.groupby(['월', '참조판례']).sum().reset_index()\n",
    "monthly_statutes = reference_statutes.groupby(['월', '참조조문']).sum().reset_index()\n",
    "\n",
    "# 결과 확인\n",
    "print(monthly_cases.head())\n",
    "print(monthly_statutes.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 시각화\n",
    "이제 집계된 결과를 바탕으로 시계열 데이터를 시각화합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 참조판례 시각화\n",
    "plt.figure(figsize=(14, 7))\n",
    "for case in monthly_cases['참조판례'].unique():\n",
    "    subset = monthly_cases[monthly_cases['참조판례'] == case]\n",
    "    plt.plot(subset['월'].dt.to_timestamp(), subset['빈도수'], marker='o', label=case)\n",
    "\n",
    "plt.title('사건번호 및 선고일자에 따른 참조판례 변화 시계열')\n",
    "plt.xlabel('선고일자')\n",
    "plt.ylabel('빈도수')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 참조조문 시각화\n",
    "plt.figure(figsize=(14, 7))\n",
    "for statute in monthly_statutes['참조조문'].unique():\n",
    "    subset = monthly_statutes[monthly_statutes['참조조문'] == statute]\n",
    "    plt.plot(subset['월'].dt.to_timestamp(), subset['빈도수'], marker='o', label=statute)\n",
    "\n",
    "plt.title('사건번호 및 선고일자에 따른 참조조문 변화 시계열')\n",
    "plt.xlabel('선고일자')\n",
    "plt.ylabel('빈도수')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 시간 혹은 날짜에 대한 정보 도출\n",
    "각 참조판례와 참조조문에서 시간이나 날짜에 대한 정보를 도출하기 위해 날짜 관련 메타데이터를 추출할 수 있습니다.\n",
    "\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 시간 및 날짜 정보 추출 함수\n",
    "def extract_date_info(ref_column):\n",
    "    date_info = []\n",
    "    for ref in ref_column:\n",
    "        # 참조판례 혹은 참조조문에서 날짜 정보 추출 (여기서는 예시로 'YYYY-MM-DD' 형식을 찾음)\n",
    "        dates = re.findall(r'\\d{4}-\\d{2}-\\d{2}', ref)\n",
    "        date_info.extend(dates)\n",
    "    return date_info\n",
    "\n",
    "# 참조판례 및 참조조문에서 날짜 정보 추출\n",
    "case_dates = extract_date_info(data['참조판례'])\n",
    "statute_dates = extract_date_info(data['참조조문'])\n",
    "\n",
    "# 결과 확인\n",
    "print(\"참조판례에서 추출된 날짜 정보:\", case_dates)\n",
    "print(\"참조조문에서 추출된 날짜 정보:\", statute_dates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 순서"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
