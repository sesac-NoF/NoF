{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 로드 및 기본 정보 확인\n",
    "텍스트 데이터를 불러와서 크기, 샘플, 결측치 여부 등을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 예시: 판례 데이터를 포함하는 CSV 파일 불러오기\n",
    "df = pd.read_csv('court_cases.csv')\n",
    "\n",
    "# 데이터의 크기 확인\n",
    "print(f\"데이터 크기: {df.shape}\")\n",
    "\n",
    "# 데이터의 상위 5개 샘플 확인\n",
    "print(df.head())\n",
    "\n",
    "# 결측치 확인\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 텍스트 길이 분포\n",
    "텍스트 데이터의 길이(단어 수 또는 글자 수)를 확인하면, 텍스트 데이터의 범위를 이해하고, 모델의 입력으로 사용될 적절한 시퀀스 길이를 결정하는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 각 텍스트의 길이(단어 수) 계산\n",
    "df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# 히스토그램으로 텍스트 길이 분포 확인\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['text_length'], bins=50, color='skyblue')\n",
    "plt.title('텍스트 길이 분포 (단어 수 기준)')\n",
    "plt.xlabel('단어 수')\n",
    "plt.ylabel('문서 수')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 단어 빈도 분석\n",
    "데이터에서 자주 사용되는 단어를 확인하면, 특정 단어가 텍스트 데이터에서 차지하는 비중을 알 수 있으며, 중요한 키워드를 발견하는 데 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 불용어 설정\n",
    "stop_words = set(stopwords.words('korean'))\n",
    "\n",
    "# 모든 텍스트에서 단어 추출 및 빈도 계산\n",
    "all_words = ' '.join(df['text']).split()\n",
    "filtered_words = [word for word in all_words if word not in stop_words]\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "# 상위 10개 자주 등장하는 단어 확인\n",
    "common_words = word_freq.most_common(10)\n",
    "print(\"자주 등장하는 단어:\", common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 한국어\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "# 한국어 텍스트에서 명사 추출\n",
    "df['nouns'] = df['text'].apply(lambda x: okt.nouns(x))\n",
    "\n",
    "# 모든 문서에서 추출한 명사를 합침\n",
    "all_words = [word for words in df['nouns'] for word in words]\n",
    "\n",
    "# 단어 빈도 계산\n",
    "word_freq = Counter(all_words)\n",
    "\n",
    "# 상위 10개 자주 등장하는 단어 확인\n",
    "common_words = word_freq.most_common(10)\n",
    "print(\"자주 등장하는 단어:\", common_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. WordCloud 시각화\n",
    "단어 빈도 분석을 바탕으로 자주 등장하는 단어를 시각화하여 직관적으로 데이터를 이해할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# 단어 빈도 기반 워드클라우드 생성\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "# 워드클라우드 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 한국어\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# 한국어 글꼴을 지정하여 워드클라우드 생성 (글꼴 경로는 시스템에 맞게 수정)\n",
    "wordcloud = WordCloud(font_path='/Library/Fonts/AppleGothic.ttf', width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 클래스 불균형 확인\n",
    "분류 작업일 경우, 각 클래스(예: 판결 결과, 적용 법률)의 불균형 여부를 확인하는 것이 중요합니다. 특정 클래스에 편향되지 않도록 데이터를 처리해야 하기 때문입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 클래스 분포 확인 (예: 'label' 컬럼에 판결 결과가 있다고 가정)\n",
    "df['label'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('클래스 분포')\n",
    "plt.xlabel('클래스')\n",
    "plt.ylabel('문서 수')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. N-그램 분석\n",
    "N-그램 분석은 단어 쌍(또는 세 단어)의 패턴을 찾는 데 유용합니다. 예를 들어, 두 단어 이상이 자주 함께 등장하는지 확인할 수 있습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 2-그램(바이그램) 분석\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=stop_words)\n",
    "X2 = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# 상위 10개의 자주 등장하는 바이그램 출력\n",
    "bi_grams = vectorizer.get_feature_names_out()\n",
    "bi_gram_counts = X2.toarray().sum(axis=0)\n",
    "bi_gram_freq = dict(zip(bi_grams, bi_gram_counts))\n",
    "sorted_bi_grams = sorted(bi_gram_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"상위 10개 바이그램:\", sorted_bi_grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 한국어\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 한국어 텍스트를 형태소 분석으로 나눈 후, 이를 이용해 2-그램 분석\n",
    "df['tokenized_text'] = df['text'].apply(lambda x: ' '.join(okt.morphs(x)))\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))  # 바이그램\n",
    "X2 = vectorizer.fit_transform(df['tokenized_text'])\n",
    "\n",
    "# 상위 10개의 자주 등장하는 바이그램 출력\n",
    "bi_grams = vectorizer.get_feature_names_out()\n",
    "bi_gram_counts = X2.toarray().sum(axis=0)\n",
    "bi_gram_freq = dict(zip(bi_grams, bi_gram_counts))\n",
    "sorted_bi_grams = sorted(bi_gram_freq.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"상위 10개 바이그램:\", sorted_bi_grams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 단어의 상관관계 분석\n",
    "단어 간의 상관관계를 분석하면 특정 단어가 함께 나타나는 패턴을 발견할 수 있습니다. 상관관계는 문맥 상 단어의 관계를 파악하는 데 유용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# 샘플 단어 목록\n",
    "words_to_check = ['법률', '판례', '형법', '민법']\n",
    "\n",
    "# 단어 상관관계 계산을 위한 데이터 프레임 생성\n",
    "word_matrix = np.zeros((len(df), len(words_to_check)))\n",
    "\n",
    "for i, text in enumerate(df['text']):\n",
    "    for j, word in enumerate(words_to_check):\n",
    "        word_matrix[i, j] = text.split().count(word)\n",
    "\n",
    "# 단어 간 상관관계 계산\n",
    "correlation_matrix = np.corrcoef(word_matrix.T)\n",
    "\n",
    "# 상관관계 히트맵 시각화\n",
    "sns.heatmap(correlation_matrix, annot=True, xticklabels=words_to_check, yticklabels=words_to_check)\n",
    "plt.title('단어 간 상관관계')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 한국어\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# 단어 상관관계를 분석할 단어 리스트\n",
    "words_to_check = ['법률', '판례', '형법', '민법']\n",
    "\n",
    "# 각 단어가 문서에서 얼마나 자주 등장하는지 카운팅\n",
    "word_matrix = np.zeros((len(df), len(words_to_check)))\n",
    "\n",
    "for i, text in enumerate(df['text']):\n",
    "    for j, word in enumerate(words_to_check):\n",
    "        word_matrix[i, j] = text.count(word)\n",
    "\n",
    "# 단어 간 상관관계 계산\n",
    "correlation_matrix = np.corrcoef(word_matrix.T)\n",
    "\n",
    "# 히트맵으로 상관관계 시각화\n",
    "sns.heatmap(correlation_matrix, annot=True, xticklabels=words_to_check, yticklabels=words_to_check)\n",
    "plt.title('단어 간 상관관계')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 텍스트 분포 및 중복 확인\n",
    "텍스트가 반복되거나 중복된 문서가 있는지 확인하는 것이 필요합니다. 중복된 데이터가 너무 많으면 학습에 부정적인 영향을 미칠 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 중복된 텍스트 확인\n",
    "duplicates = df[df.duplicated(subset='text', keep=False)]\n",
    "print(f\"중복된 문서 수: {duplicates.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 감정 분석 또는 주제 모델링\n",
    "판례나 소장 데이터를 기반으로 특정 감정(긍정, 부정) 또는 주제(상속, 형사 사건 등)를 파악할 수 있습니다. 이는 주제 모델링 또는 감정 분석으로 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF 기반 벡터화\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words=stop_words)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# LDA를 통한 주제 모델링\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(X_tfidf)\n",
    "\n",
    "# 각 문서에 대한 주제 분포 확인\n",
    "topics = lda.transform(X_tfidf)\n",
    "print(\"문서별 주요 주제 분포:\", topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
