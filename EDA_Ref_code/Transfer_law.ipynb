{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전이학습을 이용한 한국어 법률 분야의 감정 분석과 문장 유사도 분석을 위한 파인튜닝을 추가로 설명하겠습니다. 이 코드 예제는 사전 학습된 한국어 모델을 사용하여 감정 분석 및 문장 유사도 측정을 수행\n",
    "1. 전이학습 개요\n",
    "전이학습은 주로 사전 학습된 모델을 사용하여 특정 태스크에 맞게 재학습하는 과정을 포함합니다. 예를 들어, 감정 분석 태스크에 적합한 사전 학습된 언어 모델을 선택한 후, 법률 분야의 데이터셋을 통해 Fine-tuning(미세 조정)을 수행할 수 있습니다.\n",
    "\n",
    "2. 사전 학습된 모델 선택\n",
    "한국어에 대한 다양한 사전 학습된 모델이 있습니다. 대표적으로 다음과 같은 모델들이 있습니다:\n",
    "\n",
    "KoBERT: 한국어에 특화된 BERT 모델로, 법률 및 일반 텍스트 분석에 효과적입니다.\n",
    "DistilBERT: BERT의 경량화 모델로, 속도가 빠르며 메모리 효율성이 뛰어납니다.\n",
    "GPT-2: 자연어 생성에 강점을 가진 모델로, 다양한 태스크에 응용 가능합니다.\n",
    "\n",
    "3. 데이터 전처리\n",
    "경찰 범죄 분야의 법률 데이터셋을 준비합니다. 데이터는 사건의 설명, 법률 조항, 감정 레이블 등을 포함해야 합니다. 이를 위한 기본적인 전처리 단계는 다음과 같습니다:\n",
    "\n",
    "텍스트 정제: HTML 태그, 특수 문자, 숫자 등을 제거합니다.\n",
    "형태소 분석: 한국어의 어근을 추출하고 맥락을 이해하기 위해 형태소 분석기를 사용합니다. 예를 들어, KoNLPy 패키지를 사용할 수 있습니다.\n",
    "감정 레이블링: 데이터에 긍정, 부정, 중립과 같은 감정 레이블을 부여합니다.\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv('crime_data.csv')\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 텍스트 정제 및 형태소 분석\n",
    "data['cleaned_text'] = data['text'].apply(lambda x: ' '.join(okt.morphs(x)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 모델 학습\n",
    "사전 학습된 모델을 Fine-tuning하여 감정 분석을 수행합니다. 예를 들어, transformers 라이브러리를 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=3)\n",
    "\n",
    "# 데이터셋 생성\n",
    "max_len = 128\n",
    "dataset = CrimeDataset(data['cleaned_text'].tolist(), data['label'].tolist(), tokenizer, max_len)\n",
    "\n",
    "# 학습 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Trainer 초기화\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 감정 분석\n",
    "학습된 모델을 사용하여 새로운 사건 텍스트에 대한 감정을 분석합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 감정 예측 함수\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return predictions.item()\n",
    "\n",
    "# 새로운 텍스트에 대한 감정 예측\n",
    "new_text = \"사건의 내용에 대한 설명이 들어갑니다.\"\n",
    "sentiment = predict_sentiment(new_text)\n",
    "print(\"예측된 감정:\", sentiment)  # 예: 0 - 부정, 1 - 중립, 2 - 긍정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 요약 및 고려사항\n",
    "어근과 맥락: 형태소 분석을 통해 어근을 추출하고, 전이학습을 통해 맥락을 고려하여 감정 분석 모델을 학습합니다.\n",
    "데이터 다양성: 경찰 범죄 분야의 다양한 사례와 사건 설명을 포함하여 모델의 일반화 능력을 향상시킬 수 있습니다.\n",
    "사전 학습된 모델 활용: 한국어에 적합한 사전 학습된 모델을 사용하면 성능 향상에 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 파인튜닝 (Fine-tuning) 추가\n",
    "파인튜닝을 통해 사전 학습된 모델을 특정 태스크에 맞게 조정할 수 있습니다. 아래 코드는 KoBERT 모델을 사용하여 감정 분석을 위한 파인튜닝을 수행합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 감정 분석을 위한 데이터 전처리\n",
    "기본적인 데이터 전처리를 수행하고, 학습 및 검증 데이터로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# 데이터 불러오기\n",
    "data = pd.read_csv('crime_data.csv')\n",
    "\n",
    "# 형태소 분석기 초기화\n",
    "okt = Okt()\n",
    "\n",
    "# 텍스트 정제 및 형태소 분석\n",
    "data['cleaned_text'] = data['text'].apply(lambda x: ' '.join(okt.morphs(x)))\n",
    "\n",
    "# 데이터 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['cleaned_text'], data['label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 데이터셋 클래스 정의\n",
    "사전 학습된 모델을 입력으로 사용하기 위해 PyTorch의 Dataset 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3 모델 학습\n",
    "파인튜닝을 위한 학습 설정을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# 모델 및 토크나이저 불러오기\n",
    "tokenizer = BertTokenizer.from_pretrained('monologg/kobert')\n",
    "model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=3)\n",
    "\n",
    "# 데이터셋 생성\n",
    "max_len = 128\n",
    "train_dataset = CrimeDataset(X_train.tolist(), y_train.tolist(), tokenizer, max_len)\n",
    "val_dataset = CrimeDataset(X_val.tolist(), y_val.tolist(), tokenizer, max_len)\n",
    "\n",
    "# 학습 인자 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    evaluation_strategy='epoch',  # 에포크마다 평가\n",
    ")\n",
    "\n",
    "# Trainer 초기화\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 문장 유사도 분석 추가\n",
    "문장 유사도 분석을 위해 사전 학습된 BERT 모델을 사용하여 문장의 임베딩을 생성하고, 코사인 유사도를 계산합니다.\n",
    "\n",
    "2.1 문장 임베딩 생성\n",
    "문장을 임베딩으로 변환하는 함수를 정의합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_sentence_embedding(sentence, tokenizer, model):\n",
    "    inputs = tokenizer(sentence, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # [CLS] 토큰의 출력을 문장 임베딩으로 사용\n",
    "    return outputs.logits[0][0].numpy()  # 첫 번째 문장의 임베딩\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 유사도 계산\n",
    "문장 유사도를 계산하는 함수를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_similarity(sentence1, sentence2, tokenizer, model):\n",
    "    embedding1 = get_sentence_embedding(sentence1, tokenizer, model)\n",
    "    embedding2 = get_sentence_embedding(sentence2, tokenizer, model)\n",
    "    \n",
    "    # 코사인 유사도 계산\n",
    "    similarity = cosine_similarity([embedding1], [embedding2])\n",
    "    return similarity[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 예제 사용법\n",
    "이제 모델을 사용하여 감정 분석과 문장 유사도 분석을 수행할 수 있습니다.\n",
    "\n",
    "3.1 감정 분석 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 감정 예측 함수\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "    return predictions.item()\n",
    "\n",
    "# 새로운 텍스트에 대한 감정 예측\n",
    "new_text = \"사건의 내용에 대한 설명이 들어갑니다.\"\n",
    "sentiment = predict_sentiment(new_text)\n",
    "print(\"예측된 감정:\", sentiment)  # 예: 0 - 부정, 1 - 중립, 2 - 긍정\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "3.2 문장 유사도 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 유사도 계산\n",
    "sentence1 = \"피고가 범죄를 저질렀습니다.\"\n",
    "sentence2 = \"피고는 범죄를 저질렀다는 주장이 있습니다.\"\n",
    "\n",
    "similarity = calculate_similarity(sentence1, sentence2, tokenizer, model)\n",
    "print(\"문장 유사도:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "4. 요약\n",
    "이 코드는 한국어 법률 분야에서 감정 분석과 문장 유사도를 평가하기 위한 파인튜닝된 모델을 사용한 예를 보여줍니다.\n",
    "\n",
    "감정 분석: 법률 문서나 사건 설명의 감정을 분류합니다.\n",
    "문장 유사도 분석: 사건의 다양한 문장 간의 유사도를 평가하여 유사한 사건을 식별하거나 사건의 중요성을 분석합니다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
